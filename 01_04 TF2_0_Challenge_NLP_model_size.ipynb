{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF2.0 Challenge - NLP model size.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7ZZ2c26XgJk"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1BHVSQUdSdHxr-G-rlKk3JS7FupALufyn?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXkCf9hKXkIl"
      },
      "source": [
        "# Challenge: NLP model size\n",
        "1. How many parameters does the BERT base uncased model have? Use the *get_model_size function* below to help you.\n",
        "2. If you know the number of parameters for a model, how might you be able to  determine how much memory is required when running a model inference?\n",
        "3. If you wanted to run a GPT-3 175 billion inference. How much RAM would your infrastructure require.\n",
        "\n",
        "This should take you between 5-10 minutes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIea7CKmuTYB"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaHc9qEnyUT-"
      },
      "source": [
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "\n",
        "def get_model_size(checkpoint='bert-base-cased'):\n",
        "  '''For use with NLP models only\n",
        "  Usage: \n",
        "      checkpoint - the NLP model\n",
        "      returns the size of the NLP model you want to determine\n",
        "  Run this on colab.research.google.com as this downloads the model \n",
        "  before calculating the number of parameters.    \n",
        "  '''\n",
        "  \n",
        "  model = TFAutoModel.from_pretrained(checkpoint)\n",
        "  tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "  model.summary()\n",
        "\n",
        "get_model_size('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}